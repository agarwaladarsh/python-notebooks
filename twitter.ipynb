{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarwaladarsh/python-notebooks/blob/master/twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT380_PvkjWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy                   # Python wrapper around Twitter API\n",
        "from google.colab import drive  # to mount Drive to Colab notebook\n",
        "import json\n",
        "import csv\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ObvFGxQR5yY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7ec6e3a-613b-404f-ad0f-4d0d5cfa684b"
      },
      "source": [
        "# Connect Google Drive to Colab\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "# Create a variable to store the data path on your drive\n",
        "path = './gdrive/My Drive/twitter data/'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUvbU6-dSWxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Twitter API secrets from an external JSON file\n",
        "secrets = json.loads(open(path + '/secrets.json').read())\n",
        "api_key = secrets['api_key']\n",
        "api_secret_key = secrets['api_secret_key']\n",
        "access_token = secrets['access_token']\n",
        "access_token_secret = secrets['access_token_secret']\n",
        "\n",
        "# Connect to Twitter API using the secrets\n",
        "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywZH5A1KTb6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to save data into a JSON file\n",
        "# file_name: the file name of the data on Google Drive\n",
        "# file_content: the data you want to save\n",
        "def save_json(file_name, file_content):\n",
        "  with open(path + file_name, 'w', encoding='utf-8') as f:\n",
        "    json.dump(file_content, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kbe26AQUWLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to handle twitter API rate limit\n",
        "def limit_handled(cursor, list_name):\n",
        "  while True:\n",
        "    try:\n",
        "      yield cursor.next()\n",
        "    # Catch Twitter API rate limit exception and wait for 15 minutes\n",
        "    except tweepy.RateLimitError:\n",
        "      print(\"\\nData points in list = {}\".format(len(list_name)))\n",
        "      print('Hit Twitter API rate limit.')\n",
        "      for i in range(3, 0, -1):\n",
        "        print(\"Wait for {} mins.\".format(i * 5))\n",
        "        time.sleep(5 * 60)\n",
        "    # Catch any other Twitter API exceptions\n",
        "    except tweepy.error.TweepError:\n",
        "      print('\\nCaught TweepError exception' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZqQzOhCXFd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to get all tweets of a specified user\n",
        "# NOTE:This method only allows access to the most recent 3200 tweets\n",
        "# Source: https://gist.github.com/yanofsky/5436496\n",
        "def get_all_tweets(screen_name):\n",
        "  # initialize a list to hold all the Tweets\n",
        "  alltweets = []\n",
        "  # make initial request for most recent tweets \n",
        "  # (200 is the maximum allowed count)\n",
        "  new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "  # save most recent tweets\n",
        "  alltweets.extend(new_tweets)\n",
        "  # save the id of the oldest tweet less one to avoid duplication\n",
        "  oldest = alltweets[-1].id - 1\n",
        "  # keep grabbing tweets until there are no tweets left\n",
        "  while len(new_tweets) > 0:\n",
        "    print(\"getting tweets before %s\" % (oldest))\n",
        "    # all subsequent requests use the max_id param to prevent\n",
        "    # duplicates\n",
        "    new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "    # save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "    # update the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "    print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
        "    ### END OF WHILE LOOP ###\n",
        "  # transform the tweepy tweets into a 2D array that will \n",
        "  # populate the csv\n",
        "  outtweets = [[tweet.id_str, tweet.created_at, tweet.text, tweet.favorite_count,tweet.in_reply_to_screen_name, tweet.retweeted] for tweet in alltweets]\n",
        "  # write the csv\n",
        "  with open(path + '%s_tweets.csv' % screen_name, 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\",\"created_at\",\"text\",\"likes\",\"in reply to\",\"retweeted\"])\n",
        "    writer.writerows(outtweets)\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg8lDpT7YMwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to save follower objects in a JSON file.\n",
        "def get_followers():\n",
        "  \n",
        "  # Create a list to store follower data\n",
        "  followers_list = []\n",
        "  # For-loop to iterate over tweepy cursors\n",
        "  cursor = tweepy.Cursor(api.followers, count=200).pages()\n",
        "  for i, page in enumerate(limit_handled(cursor, followers_list)):  \n",
        "    print(\"\\r\"+\"Loading\"+ i % 5 *\".\", end='')\n",
        "    \n",
        "    # Add latest batch of follower data to the list\n",
        "    followers_list += page\n",
        "  \n",
        "  # Extract the follower information\n",
        "  followers_list = [x._json for x in followers_list]\n",
        "  # Save the data in a JSON file\n",
        "  save_json('followers_data.json', followers_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6QLHpzQbG1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to save friend objects in a JSON file.\n",
        "def get_friends():\n",
        "  \n",
        "  # Create a list to store friends data\n",
        "  friends_list = []\n",
        "  # For-loop to iterate over tweepy cursors\n",
        "  cursor = tweepy.Cursor(api.friends, count=200).pages()\n",
        "  for i, page in enumerate(limit_handled(cursor, friends_list)):  \n",
        "    print(\"\\r\"+\"Loading\"+ i % 5 *\".\", end='')\n",
        "    \n",
        "    # Add latest batch of friend data to the list\n",
        "    friends_list += page\n",
        "  \n",
        "  # Extract the friends information\n",
        "  friends_list = [x._json for x in friends_list]\n",
        "  # Save the data in a JSON file\n",
        "  save_json('friends_data.json', friends_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2uqmz5bcai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to save daily follower and following counts in a JSON file\n",
        "def todays_stats(dict_name):\n",
        "  # Get my account information\n",
        "  info = api.me()\n",
        "  # Get follower and following counts\n",
        "  followers_cnt = info.followers_count  \n",
        "  following_cnt = info.friends_count\n",
        "  # Get today's date\n",
        "  today = date.today()\n",
        "  d = today.strftime(\"%b %d, %Y\")\n",
        "  # Save today's stats only if they haven't been collected before\n",
        "  if d not in dict_name:\n",
        "    dict_name[d] = {\"followers\":followers_cnt, \"following\":following_cnt}\n",
        "    save_json(\"follower_history.json\", dict_name)\n",
        "  else:\n",
        "    print('Today\\'s stats already exist')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGGS7IIbbi3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myinfo = api.me()._json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLLmpUKMcZHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4aa20b53-efac-4d74-e871-27f8e3ac58ca"
      },
      "source": [
        "get_all_tweets('agarwaladarsh2')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting tweets before 1206662631446196223\n",
            "...35 tweets downloaded so far\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}